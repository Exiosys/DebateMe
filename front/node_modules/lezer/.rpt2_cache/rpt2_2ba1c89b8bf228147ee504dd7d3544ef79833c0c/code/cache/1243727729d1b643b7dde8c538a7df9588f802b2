{"code":"var StringStream = /** @class */ (function () {\n    function StringStream(string) {\n        this.string = string;\n        this.pos = 0;\n        this.token = -1;\n        this.tokenEnd = -1;\n    }\n    Object.defineProperty(StringStream.prototype, \"length\", {\n        get: function () { return this.string.length; },\n        enumerable: true,\n        configurable: true\n    });\n    StringStream.prototype.next = function () {\n        if (this.pos == this.string.length)\n            return -1;\n        return this.string.charCodeAt(this.pos++);\n    };\n    StringStream.prototype.accept = function (term, pos) {\n        if (pos === void 0) { pos = this.pos; }\n        this.token = term;\n        this.tokenEnd = pos;\n    };\n    StringStream.prototype.goto = function (n) {\n        this.pos = this.tokenEnd = n;\n        this.token = -1;\n        return this;\n    };\n    StringStream.prototype.read = function (from, to) { return this.string.slice(from, to); };\n    return StringStream;\n}());\nexport { StringStream };\nvar TokenGroup = /** @class */ (function () {\n    function TokenGroup(data, id) {\n        this.data = data;\n        this.id = id;\n    }\n    TokenGroup.prototype.token = function (input, stack) { token(this.data, input, stack, this.id); };\n    return TokenGroup;\n}());\nexport { TokenGroup };\nvar ExternalTokenizer = /** @class */ (function () {\n    function ExternalTokenizer(token, options) {\n        if (options === void 0) { options = {}; }\n        this.token = token;\n        this.contextual = options && options.contextual || false;\n    }\n    return ExternalTokenizer;\n}());\nexport { ExternalTokenizer };\nfunction token(data, input, stack, group) {\n    var state = 0, groupMask = 1 << group;\n    scan: for (;;) {\n        var array = data[state], accEnd = (array[1] << 1) + 2;\n        // Check whether this state can lead to a token in the current group\n        if ((groupMask & array[0]) == 0)\n            break;\n        // Accept tokens in this state, possibly overwriting\n        // lower-precedence / shorter tokens\n        for (var i = 2; i < accEnd; i += 2)\n            if ((array[i + 1] & groupMask) > 0) {\n                var term = array[i];\n                if (input.token == -1 || input.token == term || mayOverride(stack.parser.tokenPrec, term, input.token)) {\n                    input.accept(term);\n                    break;\n                }\n            }\n        var next = input.next();\n        for (var i = accEnd; i < array.length; i += 3) { // FIXME binary search, multiple table forms\n            var from = array[i], to = array[i + 1];\n            if (next >= from && next < to) {\n                state = array[i + 2];\n                continue scan;\n            }\n        }\n        break;\n    }\n}\nfunction mayOverride(precedences, token, prev) {\n    var iPrev = precedences.indexOf(prev);\n    return iPrev < 0 || precedences.indexOf(token) < iPrev;\n}\n//# sourceMappingURL=token.js.map","map":"{\"version\":3,\"file\":\"token.js\",\"sourceRoot\":\"\",\"sources\":[\"../../src/token.ts\"],\"names\":[],\"mappings\":\"AAaA;IAKE,sBAAqB,MAAc;QAAd,WAAM,GAAN,MAAM,CAAQ;QAJnC,QAAG,GAAG,CAAC,CAAA;QACP,UAAK,GAAG,CAAC,CAAC,CAAA;QACV,aAAQ,GAAG,CAAC,CAAC,CAAA;IAEyB,CAAC;IAEvC,sBAAI,gCAAM;aAAV,cAAe,OAAO,IAAI,CAAC,MAAM,CAAC,MAAM,CAAA,CAAC,CAAC;;;OAAA;IAE1C,2BAAI,GAAJ;QACE,IAAI,IAAI,CAAC,GAAG,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM;YAAE,OAAO,CAAC,CAAC,CAAA;QAC7C,OAAO,IAAI,CAAC,MAAM,CAAC,UAAU,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,CAAA;IAC3C,CAAC;IAED,6BAAM,GAAN,UAAO,IAAY,EAAE,GAAc;QAAd,oBAAA,EAAA,MAAM,IAAI,CAAC,GAAG;QACjC,IAAI,CAAC,KAAK,GAAG,IAAI,CAAA;QACjB,IAAI,CAAC,QAAQ,GAAG,GAAG,CAAA;IACrB,CAAC;IAED,2BAAI,GAAJ,UAAK,CAAS;QACZ,IAAI,CAAC,GAAG,GAAG,IAAI,CAAC,QAAQ,GAAG,CAAC,CAAA;QAC5B,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,CAAA;QACf,OAAO,IAAI,CAAA;IACb,CAAC;IAED,2BAAI,GAAJ,UAAK,IAAY,EAAE,EAAU,IAAY,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,CAAC,CAAA,CAAC,CAAC;IAC/E,mBAAC;AAAD,CAAC,AA1BD,IA0BC;;AAMD;IACE,oBAAqB,IAAoC,EAAW,EAAU;QAAzD,SAAI,GAAJ,IAAI,CAAgC;QAAW,OAAE,GAAF,EAAE,CAAQ;IAAG,CAAC;IAElF,0BAAK,GAAL,UAAM,KAAkB,EAAE,KAAY,IAAI,KAAK,CAAC,IAAI,CAAC,IAAI,EAAE,KAAK,EAAE,KAAK,EAAE,IAAI,CAAC,EAAE,CAAC,CAAA,CAAC,CAAC;IACrF,iBAAC;AAAD,CAAC,AAJD,IAIC;;AAED;IAGE,2BAAqB,KAAiD,EAC1D,OAAoC;QAApC,wBAAA,EAAA,YAAoC;QAD3B,UAAK,GAAL,KAAK,CAA4C;QAEpE,IAAI,CAAC,UAAU,GAAG,OAAO,IAAI,OAAO,CAAC,UAAU,IAAI,KAAK,CAAA;IAC1D,CAAC;IACH,wBAAC;AAAD,CAAC,AAPD,IAOC;;AAED,SAAS,KAAK,CAAC,IAAoC,EACpC,KAAkB,EAClB,KAAY,EACZ,KAAa;IAC1B,IAAI,KAAK,GAAG,CAAC,EAAE,SAAS,GAAG,CAAC,IAAI,KAAK,CAAA;IACrC,IAAI,EAAE,SAAS;QACb,IAAI,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC,EAAE,MAAM,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,CAAA;QACrD,oEAAoE;QACpE,IAAI,CAAC,SAAS,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;YAAE,MAAK;QACtC,oDAAoD;QACpD,oCAAoC;QACpC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,CAAC;YAAE,IAAI,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,SAAS,CAAC,GAAG,CAAC,EAAE;gBACtE,IAAI,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,CAAA;gBACnB,IAAI,KAAK,CAAC,KAAK,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,KAAK,IAAI,IAAI,IAAI,WAAW,CAAC,KAAK,CAAC,MAAM,CAAC,SAAS,EAAE,IAAI,EAAE,KAAK,CAAC,KAAK,CAAC,EAAE;oBACtG,KAAK,CAAC,MAAM,CAAC,IAAI,CAAC,CAAA;oBAClB,MAAK;iBACN;aACF;QACD,IAAI,IAAI,GAAG,KAAK,CAAC,IAAI,EAAE,CAAA;QACvB,KAAK,IAAI,CAAC,GAAG,MAAM,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,CAAC,IAAI,CAAC,EAAE,EAAE,4CAA4C;YAC3F,IAAI,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,GAAG,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,CAAA;YACtC,IAAI,IAAI,IAAI,IAAI,IAAI,IAAI,GAAG,EAAE,EAAE;gBAAE,KAAK,GAAG,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;gBAAC,SAAS,IAAI,CAAA;aAAE;SACvE;QACD,MAAK;KACN;AACH,CAAC;AAED,SAAS,WAAW,CAAC,WAAqB,EAAE,KAAa,EAAE,IAAY;IACrE,IAAI,KAAK,GAAG,WAAW,CAAC,OAAO,CAAC,IAAI,CAAC,CAAA;IACrC,OAAO,KAAK,GAAG,CAAC,IAAI,WAAW,CAAC,OAAO,CAAC,KAAK,CAAC,GAAG,KAAK,CAAA;AACxD,CAAC\"}","dts":{"name":"/home/marijn/src/lezer/lezer/token.d.ts","writeByteOrderMark":false,"text":"import { Stack } from \"./stack\";\nexport interface InputStream {\n    pos: number;\n    length: number;\n    next(): number;\n    accept(term: number, pos?: number): void;\n    token: number;\n    tokenEnd: number;\n    goto(n: number): InputStream;\n    read(from: number, to: number): string;\n}\nexport declare class StringStream implements InputStream {\n    readonly string: string;\n    pos: number;\n    token: number;\n    tokenEnd: number;\n    constructor(string: string);\n    readonly length: number;\n    next(): number;\n    accept(term: number, pos?: number): void;\n    goto(n: number): this;\n    read(from: number, to: number): string;\n}\nexport interface Tokenizer {\n    token(input: InputStream, stack: Stack): void;\n}\nexport declare class TokenGroup implements Tokenizer {\n    readonly data: readonly (readonly number[])[];\n    readonly id: number;\n    constructor(data: readonly (readonly number[])[], id: number);\n    token(input: InputStream, stack: Stack): void;\n}\nexport declare class ExternalTokenizer {\n    readonly token: (input: InputStream, stack: Stack) => void;\n    contextual: boolean;\n    constructor(token: (input: InputStream, stack: Stack) => void, options?: {\n        contextual?: boolean;\n    });\n}\n"}}

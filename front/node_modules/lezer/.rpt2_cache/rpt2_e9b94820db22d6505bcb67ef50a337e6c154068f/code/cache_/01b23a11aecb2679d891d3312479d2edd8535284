{"code":"var StringStream = /** @class */ (function () {\n    function StringStream(string) {\n        this.string = string;\n        this.pos = 0;\n        this.token = -1;\n        this.tokenEnd = -1;\n    }\n    Object.defineProperty(StringStream.prototype, \"length\", {\n        get: function () { return this.string.length; },\n        enumerable: true,\n        configurable: true\n    });\n    StringStream.prototype.next = function () {\n        if (this.pos == this.string.length)\n            return -1;\n        return this.string.charCodeAt(this.pos++);\n    };\n    StringStream.prototype.peek = function (pos) {\n        if (pos === void 0) { pos = this.pos; }\n        return pos < 0 || pos >= this.string.length ? -1 : this.string.charCodeAt(pos);\n    };\n    StringStream.prototype.accept = function (term, pos) {\n        if (pos === void 0) { pos = this.pos; }\n        this.token = term;\n        this.tokenEnd = pos;\n    };\n    StringStream.prototype.goto = function (n) {\n        this.pos = this.tokenEnd = n;\n        this.token = -1;\n        return this;\n    };\n    StringStream.prototype.read = function (from, to) { return this.string.slice(from, to); };\n    return StringStream;\n}());\nexport { StringStream };\nvar TokenGroup = /** @class */ (function () {\n    function TokenGroup(data, id) {\n        this.data = data;\n        this.id = id;\n    }\n    TokenGroup.prototype.token = function (input, stack) { token(this.data, input, stack, this.id); };\n    return TokenGroup;\n}());\nexport { TokenGroup };\nTokenGroup.prototype.contextual = false;\nvar ExternalTokenizer = /** @class */ (function () {\n    function ExternalTokenizer(token, options) {\n        if (options === void 0) { options = {}; }\n        this.token = token;\n        this.contextual = options && options.contextual || false;\n    }\n    return ExternalTokenizer;\n}());\nexport { ExternalTokenizer };\nfunction token(data, input, stack, group) {\n    var state = 0, groupMask = 1 << group;\n    scan: for (;;) {\n        if ((groupMask & data[state]) == 0)\n            break;\n        var accEnd = data[state + 1];\n        // Check whether this state can lead to a token in the current group\n        // Accept tokens in this state, possibly overwriting\n        // lower-precedence / shorter tokens\n        for (var i = state + 3; i < accEnd; i += 2)\n            if ((data[i + 1] & groupMask) > 0) {\n                var term = data[i];\n                if (input.token == -1 || input.token == term || stack.cx.parser.overrides(term, input.token)) {\n                    input.accept(term);\n                    break;\n                }\n            }\n        var next = input.next(), end = data[state + 2];\n        for (var i = accEnd; i < end; i += 3) { // FIXME binary search, multiple table forms\n            var from = data[i], to = data[i + 1];\n            if (next >= from && next < to) {\n                state = data[i + 2];\n                continue scan;\n            }\n        }\n        break;\n    }\n}\n//# sourceMappingURL=token.js.map","map":"{\"version\":3,\"file\":\"token.js\",\"sourceRoot\":\"\",\"sources\":[\"../../src/token.ts\"],\"names\":[],\"mappings\":\"AAcA;IAKE,sBAAqB,MAAc;QAAd,WAAM,GAAN,MAAM,CAAQ;QAJnC,QAAG,GAAG,CAAC,CAAA;QACP,UAAK,GAAG,CAAC,CAAC,CAAA;QACV,aAAQ,GAAG,CAAC,CAAC,CAAA;IAEyB,CAAC;IAEvC,sBAAI,gCAAM;aAAV,cAAe,OAAO,IAAI,CAAC,MAAM,CAAC,MAAM,CAAA,CAAC,CAAC;;;OAAA;IAE1C,2BAAI,GAAJ;QACE,IAAI,IAAI,CAAC,GAAG,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM;YAAE,OAAO,CAAC,CAAC,CAAA;QAC7C,OAAO,IAAI,CAAC,MAAM,CAAC,UAAU,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,CAAA;IAC3C,CAAC;IAED,2BAAI,GAAJ,UAAK,GAAc;QAAd,oBAAA,EAAA,MAAM,IAAI,CAAC,GAAG;QACjB,OAAO,GAAG,GAAG,CAAC,IAAI,GAAG,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC,UAAU,CAAC,GAAG,CAAC,CAAA;IAChF,CAAC;IAED,6BAAM,GAAN,UAAO,IAAY,EAAE,GAAc;QAAd,oBAAA,EAAA,MAAM,IAAI,CAAC,GAAG;QACjC,IAAI,CAAC,KAAK,GAAG,IAAI,CAAA;QACjB,IAAI,CAAC,QAAQ,GAAG,GAAG,CAAA;IACrB,CAAC;IAED,2BAAI,GAAJ,UAAK,CAAS;QACZ,IAAI,CAAC,GAAG,GAAG,IAAI,CAAC,QAAQ,GAAG,CAAC,CAAA;QAC5B,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,CAAA;QACf,OAAO,IAAI,CAAA;IACb,CAAC;IAED,2BAAI,GAAJ,UAAK,IAAY,EAAE,EAAU,IAAY,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,CAAC,CAAA,CAAC,CAAC;IAC/E,mBAAC;AAAD,CAAC,AA9BD,IA8BC;;AAOD;IAGE,oBAAqB,IAA2B,EAAW,EAAU;QAAhD,SAAI,GAAJ,IAAI,CAAuB;QAAW,OAAE,GAAF,EAAE,CAAQ;IAAG,CAAC;IAEzE,0BAAK,GAAL,UAAM,KAAkB,EAAE,KAAY,IAAI,KAAK,CAAC,IAAI,CAAC,IAAI,EAAE,KAAK,EAAE,KAAK,EAAE,IAAI,CAAC,EAAE,CAAC,CAAA,CAAC,CAAC;IACrF,iBAAC;AAAD,CAAC,AAND,IAMC;;AAED,UAAU,CAAC,SAAS,CAAC,UAAU,GAAG,KAAK,CAAA;AAEvC;IAGE,2BAAqB,KAAiD,EAC1D,OAAoC;QAApC,wBAAA,EAAA,YAAoC;QAD3B,UAAK,GAAL,KAAK,CAA4C;QAEpE,IAAI,CAAC,UAAU,GAAG,OAAO,IAAI,OAAO,CAAC,UAAU,IAAI,KAAK,CAAA;IAC1D,CAAC;IACH,wBAAC;AAAD,CAAC,AAPD,IAOC;;AAED,SAAS,KAAK,CAAC,IAA2B,EAC3B,KAAkB,EAClB,KAAY,EACZ,KAAa;IAC1B,IAAI,KAAK,GAAG,CAAC,EAAE,SAAS,GAAG,CAAC,IAAI,KAAK,CAAA;IACrC,IAAI,EAAE,SAAS;QACb,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC;YAAE,MAAK;QACzC,IAAI,MAAM,GAAG,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,CAAA;QAC5B,oEAAoE;QACpE,oDAAoD;QACpD,oCAAoC;QACpC,KAAK,IAAI,CAAC,GAAG,KAAK,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,CAAC;YAAE,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,SAAS,CAAC,GAAG,CAAC,EAAE;gBAC7E,IAAI,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAA;gBAClB,IAAI,KAAK,CAAC,KAAK,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,KAAK,IAAI,IAAI,IAAI,KAAK,CAAC,EAAE,CAAC,MAAM,CAAC,SAAS,CAAC,IAAI,EAAE,KAAK,CAAC,KAAK,CAAC,EAAE;oBAC5F,KAAK,CAAC,MAAM,CAAC,IAAI,CAAC,CAAA;oBAClB,MAAK;iBACN;aACF;QACD,IAAI,IAAI,GAAG,KAAK,CAAC,IAAI,EAAE,EAAE,GAAG,GAAG,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,CAAA;QAC9C,KAAK,IAAI,CAAC,GAAG,MAAM,EAAE,CAAC,GAAG,GAAG,EAAE,CAAC,IAAI,CAAC,EAAE,EAAE,4CAA4C;YAClF,IAAI,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,GAAG,IAAI,CAAC,CAAC,GAAG,CAAC,CAAC,CAAA;YACpC,IAAI,IAAI,IAAI,IAAI,IAAI,IAAI,GAAG,EAAE,EAAE;gBAAE,KAAK,GAAG,IAAI,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;gBAAC,SAAS,IAAI,CAAA;aAAE;SACtE;QACD,MAAK;KACN;AACH,CAAC\"}","dts":{"name":"/home/marijn/src/lezer/lezer/token.d.ts","writeByteOrderMark":false,"text":"import { Stack } from \"./stack\";\nexport interface InputStream {\n    pos: number;\n    length: number;\n    next(): number;\n    peek(pos?: number): number;\n    accept(term: number, pos?: number): void;\n    token: number;\n    tokenEnd: number;\n    goto(n: number): InputStream;\n    read(from: number, to: number): string;\n}\nexport declare class StringStream implements InputStream {\n    readonly string: string;\n    pos: number;\n    token: number;\n    tokenEnd: number;\n    constructor(string: string);\n    readonly length: number;\n    next(): number;\n    peek(pos?: number): number;\n    accept(term: number, pos?: number): void;\n    goto(n: number): this;\n    read(from: number, to: number): string;\n}\nexport interface Tokenizer {\n    token(input: InputStream, stack: Stack): void;\n    contextual: boolean;\n}\nexport declare class TokenGroup implements Tokenizer {\n    readonly data: Readonly<Uint16Array>;\n    readonly id: number;\n    contextual: boolean;\n    constructor(data: Readonly<Uint16Array>, id: number);\n    token(input: InputStream, stack: Stack): void;\n}\nexport declare class ExternalTokenizer {\n    readonly token: (input: InputStream, stack: Stack) => void;\n    contextual: boolean;\n    constructor(token: (input: InputStream, stack: Stack) => void, options?: {\n        contextual?: boolean;\n    });\n}\n"}}
